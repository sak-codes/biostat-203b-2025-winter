---
title: "Biostat 203B Homework 4"
subtitle: "Due Mar 9 @ 11:59PM"
author: "Your Name and UID"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
---

Display machine information:
```{r}
sessionInfo()
```
Display my machine memory.
```{r}
memuse::Sys.meminfo()
```

Load database libraries and the tidyverse frontend:
```{r}
library(bigrquery)
library(dbplyr)
library(DBI)
library(gt)
library(gtsummary)
library(tidyverse)
```

## Q1. Compile the ICU cohort in HW3 from the Google BigQuery database 

Below is an outline of steps. In this homework, we exclusively work with the BigQuery database and should not use any MIMIC data files stored on our local computer. Transform data as much as possible in BigQuery database and `collect()` the tibble **only at the end of Q1.7**.

### Q1.1 Connect to BigQuery

Authenticate with BigQuery using the service account token. Please place the service account token (shared via BruinLearn) in the working directory (same folder as your qmd file). Do **not** ever add this token to your Git repository. If you do so, you will lose 50 points.

```{r}
# path to the service account token 
satoken <- "./biostat-203b-2025-winter-4e58ec6e5579.json"
# BigQuery authentication using service account
bq_auth(path = satoken)
```
Connect to BigQuery database `mimiciv_3_1` in GCP (Google Cloud Platform), using the project billing account `biostat-203b-2025-winter`.
```{r}
# connect to the BigQuery database `biostat-203b-2025-mimiciv_3_1`
con_bq <- dbConnect(
    bigrquery::bigquery(),
    project = "biostat-203b-2025-winter",
    dataset = "mimiciv_3_1",
    billing = "biostat-203b-2025-winter"
)
con_bq
```
List all tables in the `mimiciv_3_1` database.
```{r}
dbListTables(con_bq)
```

### Q1.2 `icustays` data

Connect to the `icustays` table.
```{r}
# full ICU stays table
icustays_tble <- tbl(con_bq, "icustays") |>
  arrange(subject_id, hadm_id, stay_id) |>
  collect()
  # show_query() |>
  print(icustays_tble,width = Inf)
```

### Q1.3 `admissions` data

Connect to the `admissions` table.
```{r}
admissions_tble <-tbl(con_bq, "admissions") |>
  arrange(subject_id, hadm_id) |>
  collect()
  # show_query() |>
  print(admissions_tble,width = Inf) 
```

### Q1.4 `patients` data

Connect to the `patients` table.
```{r}
patients_tble  <-tbl(con_bq, "patients") |>
  arrange(subject_id) |>
  collect()
  # show_query() |>
  print(patients_tble,width = Inf) 
```

### Q1.5 `labevents` data

Connect to the `labevents` table and retrieve a subset that only contain subjects who appear in `icustays_tble` and the lab items listed in HW3. Only keep the last lab measurements (by `storetime`) before the ICU stay and pivot lab items to become variables/columns. Write all steps in _one_ chain of pipes.
```{r}
# Load required libraries
library(bigrquery)
library(dbplyr)
library(dplyr)
library(tidyr)
library(stringr)



# Load d_labitems table and filter itemid
dlabitems_tble <- tbl(con_bq, "d_labitems") %>%
  filter(itemid %in% c(
    50912, 50971, 50983, 50902, 50882, 51221, 51301, 50931
  )) %>%
  collect()

# Query labevents from BigQuery
labs_data <- tbl(con_bq, "labevents") %>%
  select(subject_id, itemid, storetime, valuenum) %>%
  
  # Filter for itemid of interest
  filter(itemid %in% dlabitems_tble$itemid) %>%
  
  # Select necessary variables
  
  
  # Join with icustays table
  left_join(
    tbl(con_bq, "icustays") %>%
      select(subject_id, stay_id, intime),
    by = "subject_id"
  ) %>%
  
  # Filter for records before ICU intime
  filter(storetime < intime) %>%
  
  # Group by subject_id, stay_id, itemid
  group_by(subject_id, stay_id, itemid) %>%
  
  # Take the first row in each group
  slice_max(storetime, n = 1) %>%
  select(-storetime, -intime) %>%
  ungroup() %>%
  
  # Pivot wider to make itemid columns
  pivot_wider(names_from = itemid, values_from = valuenum) %>%
  
  # Rename columns based on dlabitems labels
  rename_at(
    vars(as.character(dlabitems_tble$itemid)),
    ~ str_to_lower(dlabitems_tble$label)
  ) %>%
  collect() %>% # This step pulls the data from BigQuery into R
  # Rename specific columns (e.g., white blood cells)
  rename(wbc = `white blood cells`) %>%
  
  # Collect the results into an R dataframe
  
 arrange(subject_id, stay_id)  

# Display the final dataframe
print(labs_data, width = Inf)



```


### Q1.6 `chartevents` data

Connect to `chartevents` table and retrieve a subset that only contain subjects who appear in `icustays_tble` and the chart events listed in HW3. Only keep the first chart events (by `storetime`) during ICU stay and pivot chart events to become variables/columns. Write all steps in _one_ chain of pipes. Similary to HW3, if a vital has multiple measurements at the first `storetime`, average them.

```{r}
# Assuming `con_bq` is your connection to BigQuery
chartevents_tble <- tbl(con_bq, "chartevents")

# List of vital item IDs
vitals_itemids <- c(
  220045,  # Heart rate
  220179,  # Systolic non-invasive blood pressure
  220180,  # Diastolic non-invasive blood pressure
  223761,  # Body temperature (Fahrenheit)
  220210   # Respiratory rate
)
  


# Connect to BigQuery and retrieve the data
vitals_data <- chartevents_tble %>%
  
  
  # Select necessary columns
  select(subject_id, stay_id, itemid, charttime, value, valuenum, storetime) %>%
  
  # Filter for relevant itemids and subjects present in icustays_tble
  filter(itemid %in% vitals_itemids) %>%
  inner_join(
    select(icustays_tble, subject_id, stay_id,intime,outtime),
    by = c("subject_id", "stay_id"),
    copy = TRUE
  ) %>%
  
  # Filter for valid storetimes within ICU stay
  filter(storetime >= intime & storetime <= outtime) %>%
  filter(!is.na(valuenum)) %>%
  
  # Group by subject, stay, and itemid, and select the first chart event (by storetime)
  group_by(subject_id, stay_id, itemid) %>%
  slice_min(storetime, with_ties = TRUE) %>%
  
  # Calculate the average value for each itemid at the first storetime
  summarise(mean_value = mean(valuenum, na.rm = TRUE)) %>%
  
  # Ungroup
  ungroup() %>%
  
  # Pivot the data to get itemids as columns
  pivot_wider(names_from = itemid, values_from = mean_value) %>%
  
  # Rename the columns to meaningful names
  rename(
    heart_rate = `220045`,
    non_invasive_bloodpressure_systolic = `220179`,
    non_invasive_bloodpressure_diastolic = `220180`,
    temperature_fahrenheit = `223761`,
    respiratory_rate = `220210`
  ) %>%
  
  # Round the values for better readability
  mutate(
    heart_rate = round(heart_rate, 1),
    non_invasive_bloodpressure_systolic = round(non_invasive_bloodpressure_systolic, 1),
    non_invasive_bloodpressure_diastolic = round(non_invasive_bloodpressure_diastolic, 1),
    temperature_fahrenheit = round(temperature_fahrenheit, 1),
    respiratory_rate = round(respiratory_rate, 1)
  )%>%
  
  # Collect the results
  collect() %>%
  
  # Relocate subject_id, stay_id to the front for better readability
  relocate(subject_id, stay_id, heart_rate, non_invasive_bloodpressure_diastolic,
           non_invasive_bloodpressure_systolic, respiratory_rate, temperature_fahrenheit) %>%
  
  # Arrange by subject_id and stay_id
  arrange(subject_id, stay_id) 
  
  # Print the result
  print(vitals_data,width = Inf)
```
### Q1.7 Put things together

This step is similar to Q7 of HW3. Using _one_ chain of pipes `|>` to perform following data wrangling steps: (i) start with the `icustays_tble`, (ii) merge in admissions and patients tables, (iii) keep adults only (age at ICU intime >= 18), (iv) merge in the labevents and chartevents tables, (v) `collect` the tibble, (vi) sort `subject_id`, `hadm_id`, `stay_id` and `print(width = Inf)`.

```{r}
patients_tble_local <- collect(patients_tble)
admissions_tble_local <- collect(admissions_tble)
```


```{r}
library(bigrquery)

# Make sure the data is a tibble or data frame
patients_tble <- as_tibble(patients_tble)

# Upload the local data frame to BigQuery
bq_table_upload(
  project = "biostat-203b-2025-winter",
  dataset = "biostat-203b-2025-winter",
  table = "patients_tble",  # Name of the BigQuery table to upload to
  values = patients_tble,   # The local data frame or tibble
  overwrite = TRUE          # Optional: Overwrite the table if it exists
)


```


```{r}
mimic_icu_cohort <- icustays_tble %>%
  left_join(patients_tble,by = "subject_id",copy=TRUE)  %>%
  left_join(admissions_tble, by = c("hadm_id","subject_id"),copy=TRUE) %>%
  left_join(vitals_data,by = c("subject_id","stay_id"),copy=TRUE) %>%
  left_join(labs_data,by=c("subject_id","stay_id"),copy=TRUE)%>%
  mutate(
    intime_year = year(intime),  # Extract year from ICU admission time
    age_intime = anchor_age + (intime_year - anchor_year) 
  ) %>%
  
  
  # Optionally remove the intermediate intime_year column
  select(-intime_year)

print(mimic_icu_cohort,width=Inf)
  
```

### Q1.8 Preprocessing

Perform the following preprocessing steps. (i) Lump infrequent levels into "Other" level for `first_careunit`, `last_careunit`, `admission_type`, `admission_location`, and `discharge_location`. (ii) Collapse the levels of `race` into `ASIAN`, `BLACK`, `HISPANIC`, `WHITE`, and `Other`. (iii) Create a new variable `los_long` that is `TRUE` when `los` is greater than or equal to 2 days. (iv) Summarize the data using `tbl_summary()`, stratified by `los_long`. Hint: `fct_lump_n` and `fct_collapse` from the `forcats` package are useful.

Hint: Below is a numerical summary of my tibble after preprocessing:

<iframe width=95% height="500" src="./mimic_icu_cohort_gtsummary.html"></iframe>

### Q1.9 Save the final tibble

Save the final tibble to an R data file `mimic_icu_cohort.rds` in the `mimiciv_shiny` folder.
```{r}
# make a directory mimiciv_shiny
if (!dir.exists("mimiciv_shiny")) {
  dir.create("mimiciv_shiny")
}
# save the final tibble
mimic_icu_cohort |>
  write_rds("mimiciv_shiny/mimic_icu_cohort.rds", compress = "gz")
```
Close database connection and clear workspace.
```{r}
if (exists("con_bq")) {
  dbDisconnect(con_bq)
}
rm(list = ls())
```
Although it is not a good practice to add big data files to Git, for grading purpose, please add `mimic_icu_cohort.rds` to your Git repository.

## Q2. Shiny app

Develop a Shiny app for exploring the ICU cohort data created in Q1. The app should reside in the `mimiciv_shiny` folder. The app should contain at least two tabs. One tab provides easy access to the graphical and numerical summaries of variables (demographics, lab measurements, vitals) in the ICU cohort, using the `mimic_icu_cohort.rds` you curated in Q1. The other tab allows user to choose a specific patient in the cohort and display the patient's ADT and ICU stay information as we did in Q1 of HW3, by dynamically retrieving the patient's ADT and ICU stay information from BigQuery database. Again, do **not** ever add the BigQuery token to your Git repository. If you do so, you will lose 50 points.

