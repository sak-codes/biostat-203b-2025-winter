---
title: "Biostat 203B Homework 1"
subtitle: Due Jan 24, 2025 @ 11:59PM
author: Sakshi Oza, 606542442
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
    link-external-icon: true
    link-external-newwindow: true
---

Display machine information for reproducibility:
```{r}
#| eval: true
sessionInfo()
```

## Q1. Git/GitHub

**No handwritten homework reports are accepted for this course.** We work with Git and GitHub. Efficient and abundant use of Git, e.g., frequent and well-documented commits, is an important criterion for grading your homework.

1. Apply for the [Student Developer Pack](https://education.github.com/pack) at GitHub using your UCLA email. You'll get GitHub Pro account for free (unlimited public and private repositories).

2. Create a **private** repository `biostat-203b-2025-winter` and add `Hua-Zhou` and TA team (`Tomoki-Okuno` for Lec 1; `parsajamshidian` and `BowenZhang2001` for Lec 82) as your collaborators with write permission.

3. Top directories of the repository should be `hw1`, `hw2`, ... Maintain two branches `main` and `develop`. The `develop` branch will be your main playground, the place where you develop solution (code) to homework problems and write up report. The `main` branch will be your presentation area. Submit your homework files (Quarto file `qmd`, `html` file converted by Quarto, all code and extra data sets to reproduce results) in the `main` branch.

4. After each homework due date, course reader and instructor will check out your `main` branch for grading. Tag each of your homework submissions with tag names `hw1`, `hw2`, ... Tagging time will be used as your submission time. That means if you tag your `hw1` submission after deadline, penalty points will be deducted for late submission.

5. After this course, you can make this repository public and use it to demonstrate your skill sets on job market.

### Solution 1
Q1 done

## Q2. Data ethics training

This exercise (and later in this course) uses the [MIMIC-IV data v3.1](https://physionet.org/content/mimiciv/3.1/), a freely accessible critical care database developed by the MIT Lab for Computational Physiology. Follow the instructions at <https://mimic.mit.edu/docs/gettingstarted/> to (1) complete the CITI `Data or Specimens Only Research` course and (2) obtain the PhysioNet credential for using the MIMIC-IV data. Display the verification links to your completion report and completion certificate here. **You must complete Q2 before working on the remaining questions.** (Hint: The CITI training takes a few hours and the PhysioNet credentialing takes a couple days; do not leave it to the last minute.)

### Solution 2
Here is my link for [completion certificate](https://www.citiprogram.org/verify/?wb26041d2-add6-4fd8-a9ce-cafea65fd044-67373844) and [completion report](https://www.citiprogram.org/verify/?k9dbc9776-ab32-4665-9143-5d3ca1f6c09a-67373844)

## Q3. Linux Shell Commands

### Solution 3.1

1. Make the MIMIC-IV v3.1 data available at location `~/mimic`. The output of the `ls -l ~/mimic` command should be similar to the below (from my laptop).
```{bash}
#| eval: true
# content of mimic folder
ls -l ~/mimic/
```
Refer to the documentation <https://physionet.org/content/mimiciv/3.1/> for details of data files. Do **not** put these data files into Git; they are big. Do **not** copy them into your directory. Do **not** decompress the gz data files. These create unnecessary big files and are not big-data-friendly practices. Read from the data folder `~/mimic` directly in following exercises. 

Use Bash commands to answer following questions.

2. Display the contents in the folders `hosp` and `icu` using Bash command `ls -l`. Why are these data files distributed as `.csv.gz` files instead of `.csv` (comma separated values) files? Read the page <https://mimic.mit.edu/docs/iv/> to understand what's in each folder.

### Solution 3.2

The primary reason for using .csv.gz over .csv is to improve the efficiency of file storage and transfer, especially for large datasets, without losing any data quality or integrity.Many tools and programming languages (like Python, R, and Unix-based systems) can automatically decompress .gz files  without needing additional steps from the user. This makes it convenient for those processing the data, as they can directly load and work with compressed data.

```{bash}
# Solution 3.2
ls -l ~/mimic/hosp
ls -l ~/mimic/icu
```

### Solution 3.3

3. Briefly describe what Bash commands `zcat`, `zless`, `zmore`, and `zgrep` do.

a. `zcat` - Displays the contents of compressed `.gz` files without uncompressing them.
Example usage: 
```{bash}
# The output is very long and so commenting the code.
# zcat < ~/mimic/hosp/poe.csv.gz
```

b. `zless` - Opens compressed `.gz` files for reading, similar to the less command.
```{bash}
# zless  ~/mimic/hosp/poe.csv.gz
```

c.`zmore`- Similar to zless, but works like more to display the contents of compressed .gz files page by page.
```{bash}
# zmore  ~/mimic/hosp/poe.csv.gz
```

d.`zgrep`- Searches for a pattern in compressed .gz files, similar to grep.
```{bash}
# zgrep "pattern" ~/mimic/hosp/poe.csv.gz
```

### Solution 3.4

4. (Looping in Bash) What's the output of the following bash script?

```{bash}
#| eval: false
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  ls -l $datafile
done
```
Display the number of lines in each data file using a similar loop. (Hint: combine linux commands `zcat <` and `wc -l`.)

```{bash}
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  echo "Number of lines in" $datafile "is"
  zcat < $datafile | wc -l
done
```

### Solution 3.5

5. Display the first few lines of `admissions.csv.gz`. How many rows are in this data file, excluding the header line? Each `hadm_id` identifies a hospitalization. How many hospitalizations are in this data file? How many unique patients (identified by `subject_id`) are in this data file? Do they match the number of patients listed in the `patients.csv.gz` file? (Hint: combine Linux commands `zcat <`, `head`/`tail`, `awk`, `sort`, `uniq`, `wc`, and so on.)

```{bash}
# displaying first few lines of the file
zcat < ~/mimic/hosp/admissions.csv.gz | head 
```

```{bash}
# Number of rows in the datafile excluding headers
zcat < ~/mimic/hosp/admissions.csv.gz  | tail -n +2 | wc -l

```
 
```{bash}
# number of hospitalizations
zcat < ~/mimic/hosp/admissions.csv.gz  | tail -n +2 | awk -F',' '{print $2}' |
wc -l
```

```{bash}
# Number of unique patients
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $1}' | 
sort | uniq | wc -l
```

```{bash}
# Compare unique patients in `admissions.csv.gz` and `patients.csv.gz`
zcat < ~/mimic/hosp/patients.csv.gz | tail -n +2 | awk -F',' '{print $1}' |
sort | uniq | wc -l

```

The difference in the number of unique patients between the two files suggests that not all patients in the patients.csv.gz file are represented in the admissions.csv.gz file. This could indicate that some patients were not included in the admissions.csv.gz file.

### Solution 3.6

6. What are the possible values taken by each of the variable `admission_type`, `admission_location`, `insurance`, and `ethnicity`? Also report the count for each unique value of these variables in decreasing order. (Hint: combine Linux commands `zcat`, `head`/`tail`, `awk`, `uniq -c`, `wc`, `sort`, and so on; skip the header line.)

```{bash}
# Possible values taken by admission type
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $6}' |
sort | uniq -c | sort -nr
```

```{bash}
# Possible values taken by admission_location
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $8}' | 
sort | uniq -c | sort -nr
```

```{bash}
# Possible values taken by insurance
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $10}' |
sort | uniq -c | sort -nr
```

```{bash}
# Possible values taken by ethinicity
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $13}' | 
sort | uniq -c | sort -nr
```

### Solution 3.7

7. The `icusays.csv.gz` file contains all the ICU stays during the study period. How many ICU stays, identified by `stay_id`, are in this data file? How many unique patients, identified by `subject_id`, are in this data file?

```{bash}
zcat < ~/mimic/icu/icustays.csv.gz| head
```

```{bash}
# unique number of stay_id
zcat < ~/mimic/icu/icustays.csv.gz| tail -n +2 | awk -F',' '{print $3}' | 
sort | uniq | wc -l
```

```{bash}
# unique number of subject_id
zcat < ~/mimic/icu/icustays.csv.gz | tail -n +2 | awk -F',' '{print $1}' |
sort | uniq | wc -l
```

### Solution 3.8

8. _To compress, or not to compress. That's the question._ Let's focus on the big data file `labevents.csv.gz`. Compare compressed gz file size to the uncompressed file size. Compare the run times of `zcat < ~/mimic/labevents.csv.gz | wc -l` versus `wc -l labevents.csv`. Discuss the trade off between storage and speed for big data files. (Hint: `gzip -dk < FILENAME.gz > ./FILENAME`. Remember to delete the large `labevents.csv` file after the exercise.)

```{bash}
# Measure storage of compressed file
ls -l ~/mimic/hosp/labevents.csv.gz
```

```{bash}
# Measure time of compressed file
time zcat < ~/mimic/hosp/labevents.csv.gz | wc -l
```

```{bash}
# Decompress the file
gzip -dk ~/mimic/hosp/labevents.csv.gz
```

```{bash}
# Measure storage of uncompressed file
ls -l ~/mimic/hosp/labevents.csv
```


```{bash}
# Measure time of uncompressed file
time wc -l ~/mimic/hosp/labevents.csv
```


```{bash}
# Delete the uncompressed file
rm ~/mimic/hosp/labevents.csv
```

#### Explanation

1. **Storage Comparison:**
   - **Compressed File:** The compressed file, `labevents.csv.gz`, has a size of **2.59 GB** (2,592,909,134 bytes). This is a significant reduction in size compared to the uncompressed version, making it much more storage-efficient. Compression is beneficial when dealing with large datasets, as it helps conserve disk space, which can be critical in large-scale data projects.
   - **Uncompressed File:** After decompression, the file size grows to a massive **18.4 GB** (18,402,851,720 bytes). This is typical for CSV files, which often contain large amounts of raw data. While the uncompressed file is much larger, it is easier and faster to process since there is no need to decompress it first.

2. **Time Comparison:**
   - **Compressed File (`zcat`):** The time to process the compressed file with the command `zcat < labevents.csv.gz | wc -l` took approximately **19.3 seconds (real time)**. The high **user time of 30.07 seconds** reflects the extra computational cost associated with decompressing the file on the fly, which slows down the operation compared to uncompressed data.
   - **Uncompressed File (`wc -l`):** Processing the uncompressed file with `wc -l` took about **18.6 seconds (real time)**, which is slightly faster than the compressed version. This is because the file is directly read from disk without the overhead of decompression. The **user time is much lower at 16.43 seconds**, indicating that less CPU power was needed to process the data compared to the compressed version.

## Q4. Who's popular in Price and Prejudice

### Solution 4.1

1. You and your friend just have finished reading *Pride and Prejudice* by Jane Austen. Among the four main characters in the book, Elizabeth, Jane, Lydia, and Darcy, your friend thinks that Darcy was the most mentioned. You, however, are certain it was Elizabeth. Obtain the full text of the novel from <http://www.gutenberg.org/cache/epub/42671/pg42671.txt> and save to your local folder. 

```{bash}
# Add wget PATH on mac
PATH=$PATH:/opt/homebrew/bin
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
```

Explain what `wget -nc` does. Do **not** put this text file `pg42671.txt` in Git. Complete the following loop to tabulate the number of times each of the four characters is mentioned using Linux commands.

```{bash}
for char in Elizabeth Jane Lydia Darcy
do
  echo $char:
  grep -o -i $char pg42671.txt | wc -l
done
```

#### Explanation
`wget -nc` is a command used to download files from the web using the wget tool with the -nc option, which stands for "no clobber."
`wget`: A command-line tool used to download files from the internet. It supports downloading files over HTTP, HTTPS, and FTP protocols.
`-nc` (no clobber): Prevents `wge`t from overwriting an existing file. If the file we're trying to download already exists in the directory, the -nc option ensures that `wget` does not re-download the file or overwrite it.
- If the file does not exist: wget will download the file normally.
- If the file already exists: wget will skip downloading the file and leave the existing file unchanged.
- If partial download exists: wget -nc will not continue or restart an incomplete file download.
According to this **Elizabeth** is repeated the most times (634).

### Solution 4.2

2. What's the difference between the following two commands?
```{bash}
#| eval: false
echo 'hello, world' > test1.txt
```
and
```{bash}
#| eval: false
echo 'hello, world' >> test2.txt
```

#### Explanation

a. `echo 'hello, world' > test1.txt`:
The `>` operator is used for output redirection.
This command writes `'hello, world'` to the file `test1.txt`. If the file already exists, it will be overwritten.

b. `echo 'hello, world' >> test2.txt`:
The `>>` operator is used for append redirection.
This command appends `'hello, world'` to the file `test2.txt`. If the file already exists, the new text will be added to the end of the file without deleting the existing content. If the file doesn't exist, it will create the file and then write `'hello, world'` to it.

### Solution 4.3

3. Using your favorite text editor (e.g., `vi`), type the following and save the file as `middle.sh`:
Using `chmod` to make the file executable by the owner, and run

```{bash}
#| eval: false
#!/bin/sh
# Select lines from the middle of a file.
# Usage: bash middle.sh filename end_line num_lines
head -n "$2" "$1" | tail -n "$3"
```

```{bash}
chmod +x middle.sh
./middle.sh pg42671.txt 20 5
```
Explain the output. Explain the meaning of `"$1"`, `"$2"`, and `"$3"` in this shell script. Why do we need the first line of the shell script?

#### Explanation

output : head -n 20 pg42671.txt: This command extracts the first 20 lines of pg42671.txt.
tail -n 5: From the 20 lines output by head, this command takes the last 5 lines.

The first line of the script,` #!/bin/sh`, is known as the shebang.
-It tells the operating system which interpreter to use for running the script. In this case, the shell interpreter located at `/bin/sh `is used.
-Without this line, the system might not know how to execute the script, especially if we're running it directly from the command line.

In shell scripts, `"$1"`, `"$2"`, and `"$3"` are positional parameters that correspond to the arguments passed when we run the script:
-`"$1"`: The first argument passed to the script. In this case, it's the filename (`pg42671.tx`t).
-`"$2"`: The second argument, specifying the number of lines to extract from the start of the file (used by the head command). In this case, 20.
-`"$3"`: The third argument, specifying the number of lines to extract from the bottom of the result (used by the tail command). In this case, 5.

## Q5. More fun with Linux

Try following commands in Bash and interpret the results: `cal`, `cal 2025`, `cal 9 1752` (anything unusual?), `date`, `hostname`, `arch`, `uname -a`, `uptime`, `who am i`, `who`, `w`, `id`, `last | head`, `echo {con,pre}{sent,fer}{s,ed}`, `time sleep 5`, `history | tail`.

### Solution 5

a. **`cal`**:  
   This command shows us the current month's calendar. For example, if it's January, it’ll display the calendar for January.
```{bash}
cal
```
   
b. **`cal 2025`**:  
   This will display the entire calendar for the year 2025. It's just a  way to see what days of the week certain months fall on.
```{bash}
cal 2025
```

c. **`cal 9 1752`**:  
   This one shows the calendar for September 1752. The `cal 9 1752` command is special because September 1752 had a unique situation where 11 days were skipped when the Gregorian calendar was adopted, so we’ll see gaps in the calendar.
```{bash}
cal 9 1752
```

d. **`date`**:  
   It shows the current date and time on our system. 
```{bash}
date
```

e. **`hostname`**:  
   This gives the name of my computer or server. 
```{bash}
hostname
```

f. **`arch`**:  
   Tells us what type of architecture our system is using (like whether it's a 64-bit or 32-bit computer). 
```{bash}
arch
```

g. **`uname -a`**:  
   This one gives us a whole bunch of details about our operating system, like what version of Linux you’re running, the kernel version, and more. 
```{bash}
uname -a
```

h. **`uptime`**:  
   Tells us how long our computer has been running without being rebooted, along with the current time and a little information on how busy our system is right now.
```{bash}
uptime
```

i. **`who am i`**:  
   This shows who we are , where we’re logged in from, and when we logged in.
```{bash}
who am i
```

j. **`who`**:  
   Lists all the people currently logged into our system, along with where we are logged in from and when we logged in.
```{bash}
who
```

k. **`w`**:  
   Similar to `who`, but it gives a bit more detail. It shows who’s logged in, what they’re doing, how long they’ve been idle, and the system load.
```{bash}
w
```

l. **`id`**:  
   Displays your user ID (UID), group ID (GID), and the groups you're part of. 
```{bash}
id
```

m. **`last | head`**:  
   This shows the last few logins to the system (using the `last` command). It shows you who's logged in, when they logged in, and where from, but with the `head` command, it limits it to the most recent 10.
```{bash}
last | head
```

n. **`echo {con,pre}{sent,fer}{s,ed}`**:  
   This is a  called "brace expansion" that combines different parts of words . It will create a list like:
   - cons
   - sent
   - s
   - ed
   - present
   - fer
   - s
   - ed

```{bash}
echo {con,pre}{sent,fer}{s,ed}
```

o. **`time sleep 5`**:  
   This command measures how long the `sleep 5` command takes to run. Since `sleep 5` just makes the system pause for 5 seconds, it will show you that it took exactly 5 seconds to run.
```{bash}
time sleep 5
```

p. **`history | tail`**:  
   Shows you the last few commands you've typed in your terminal. If we have been working on a project, it’s like looking back at our "command history" to see what we did recently.
```{bash}
history | tail
```

## Q6. Book

1. Git clone the repository <https://github.com/christophergandrud/Rep-Res-Book> for the book _Reproducible Research with R and RStudio_ to your local machine. Do **not** put this repository within your homework repository `biostat-203b-2025-winter`.

2. Open the project by clicking `rep-res-3rd-edition.Rproj` and compile the book by clicking `Build Book` in the `Build` panel of RStudio. (Hint: I was able to build `git_book` and `epub_book` directly. For `pdf_book`, I needed to add a line `\usepackage{hyperref}` to the file `Rep-Res-Book/rep-res-3rd-edition/latex/preabmle.tex`.)

The point of this exercise is (1) to obtain the book for free and (2) to see an example how a complicated project such as a book can be organized in a reproducible way. Use `sudo apt install PKGNAME` to install required Ubuntu packages and `tlmgr install PKGNAME` to install missing TexLive packages.

For grading purpose, include a screenshot of Section 4.1.5 of the book here.

### Solution 6

a. PDF build

![PDF](images/pdf_ss.png)

b. GitHub build

![Github](images/github_ss.png)

c. EPUB build

![EPUB](images/epub_ss.png)
